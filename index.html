<!DOCTYPE html><html lang="en"><head><title>Aljosa's Web Corner</title><link rel="stylesheet" href=https://latex.now.sh/style.min.css /></head><body><h1>Aljosa Osep, Ph.D.</h1><img src="img/aljosa.jpg" alt="In Colombia, my fave" width="200" align="left" style="padding:10px;"><p>Hi, I'm Aljosa! I am a Senior Research Scientist at NVIDIA, working on learning to understand the dynamic world from raw, unlabeled streams of sensory data. <br><br> I come from the Alpine side of Slovenia. Prior, I obtained my M.Sc. degree at the University of Bonn, and a Ph.D. from the RWTH Aachen University under the supervision of <a href="https://www.vision.rwth-aachen.de/person/1/">Prof. Bastian Leibe</a>. I have had a pleasure of working with <a href="https://dvl.in.tum.de/team/lealtaixe/">Prof. Laura Leal-Taixe</a> (<a href="https://dvl.in.tum.de/">Dynamic Vision and Learning Group</a>) at Technical University of Munich and <a href="http://www.cs.cmu.edu/~deva/">Prof. Deva Ramanan</a> at the Robotics Institute of Carnegie Mellon University.</p><br><b><a href="rss_2020_camready.html">Research Statement (2019)</a> 
	<a href="https://twitter.com/AljosaOsep">Twitter</a> 
	<a href="https://scholar.google.de/scholar?hl=en&as_sdt=0%2C5&q=aljosa+osep&oq=a">Scholar</a> <h2>News</h2><ul><li><b><i>June 2024:</i></b> I joined NVIDIA as a Senior Research Scientist!</li><li><b><i>March 2024:</i></b> Our <a href="https://arxiv.org/abs/2403.13129">paper</a> on <i>Learning to segment anything in Lidar (SAL)</i> was featured at GTC2024! <a href="https://youtu.be/LLSuUBObttE?si=WvQFphni5vX7Es5I">NVIDIA AI Tools for Autonomous Vehicle Developers</a>.</li><li><b><i>September 2022:</i></b> Two papers accepted to NeurIPS 2022! Excited to be back to NOLA soon!</a></li><li><b><i>June 2022:</i></b> I was featured in the <b>TWIMLAI podcast!</b> <a href="https://twimlai.com/podcast/twimlai/on-the-path-towards-robot-vision-with-aljosa-osep/">Link</a></li><li><b><i>August 2021:</i></b> I am one of the three persons listed as outstanding reviewers for all top-tier computer vision conferences in 2020/21, see <a href="https://twitter.com/simon_niklaus/status/1433127773409665025?s=20"> the informal analysis</a> by Simon Niklaus! Thanks to ACs for the recognition and Simon for pointing this out.</li><li><b><i>August 2021:</i></b> I was awarded Borchers Plaquette at RWTH Aachen University for outstanding doctoral dissertation!</li><li><b><i>June 2021:</i></b> I am spending a year at the Carnegie Mellon University (The Robotics Institute, <a href="https://labs.ri.cmu.edu/argo-ai-center/">CMU Argo AI Center for Autonomous Vehicle Research</a>) in Pittsburgh! Thanks to <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a> for hosting me!</a></li><li><b><i>April 2020:</i></b> Learned how to make pancakes! <a href="https://photos.app.goo.gl/uMeWKNmqPUcuwyXQ6">Evidence</a></li></ul> <h2>Students Supervised</h2><ul><li>Xindi Wu (CMU, 2022) <span>&#8594;</span> <i>Princeton</i></li><li>Vladimir Fomenko (TUM, 2022) <span>&#8594;</span> <i>OpenAI</i></li><li>Anirudh S Chakravarthy (CMU, 2022) <span>&#8594;</span> <i>Cruise AI</i></li><li>Meghana Reddy Ganesina (CMU, 2022) <span>&#8594;</span> <i>Zoox</i></li><li>Abhinav Agarwalla (CMU, 2022) <span>&#8594;</span> <i>prior Argo AI, now Neural Magic</i></li><li>Vladimir Yugay (TUM, 2022) <span>&#8594;</span> <i>University of Amsterdam</i></li><li>Alexandr Kim (TUM, 2019-2021) <span>&#8594;</span> <i>Meta</i></li><li>Yang Liu (TUM, 2020) <span>&#8594;</span> <i>Huawei</i></li><li>Manuel Kolmet (TUM, 2021) <span>&#8594;</span> <i>GLASS Imaging</i></li><li>Mehmet Aygün (TUM, 2020) <span>&#8594;</span> <i>University of Edinburgh</i></li><li>Johannes Gross (RWTH Aachen, 2019)</li><li>Deyvid Kochanov (RWTH Aachen, 2016)</li><li>Dirk Klostermann (RWTH Aachen, 2015)</li></ul> <h2>Service</h2><ul><li>Area Chair (AC) for ICLR, CVPR, ECCV, ICCV, WACV, ACCV.</li><li>I am the main organizer of <a href="https://motchallenge.net/workshops/bmtt2021">6th BMTT MOTChallenge Workshop: Segmenting and Tracking Every Point and Pixel</a> at ICCV'21 workshop, and co-organizer of: <a href="https://motchallenge.net/workshops/bmtt2022">7th Workshop on Benchmarking Multi-Target Tracking: How Far Can Synthetic Data Take us?</a> at CVPR'22, <a href="http://taodataset.org/workshop/">Tracking and its many guises Workshop</a> at ECCV'2020, <a href="https://motchallenge.net/workshops/bmtt2020">Multi-Object Tracking and Segmentation Workshop</a> at CVPR'2020.</li><li>Reviewer for (<i>machine learning, vision conferences</i>) CVPR, ECCV, ICCV, BMVC, NeurIPS, ICML, ICLR; (<i>robotics conferences</i>) ICRA, IROS, RSS; (<i>journals</i>) IJCV, RAL, TPAMI.</li><li>I am in RSS Pioneers 2021 program committee and on the ECCV'24 organization team!</li></ul> <h2>Talks</h2><ul><li><b><i>June 2024:</i></b> CVPR 2024 Area Chair Panel, invited talk: Learning To Understand The World From Video, <a href="https://docs.google.com/presentation/d/1JCy1TARAlcT0HIT07uRUic1iPsMejIDqYckeRTOvzn8/edit?usp=sharing">Slides</a></li><li><b><i>June 2023:</i></b> <a href="">CVPR 2023, Visual Perception via Learning in an Open World, invited talk: Learning To Understand The World From Video</a>, <a href="https://docs.google.com/presentation/d/1DgNPaJm6WWjgNdIjdDiyaLw5d-2Tu5hvraqWJSmGK3g/edit?usp=sharing">Slides</a></li><li><b><i>June 2023:</i></b> <a href="https://fri.uni-lj.si/sl/dogodek/gostujoce-predavanje-learning-understand-world-video">University of Ljubljana, invited talk: Learning To Understand The World From Video</a>, <a href="https://docs.google.com/presentation/d/1w__6jKhNgzwT66oMcUyKgvGJEwgOVMlH-12WAuAHaKc/edit?usp=sharing">Slides</a></li><li><b><i>February 2023:</i></b> <a href="https://fri.uni-lj.si/sl/dogodek/gostujoce-predavanje-learning-understand-world-video">University of Ljubljana, invited talk: Learning To Understand The World From Video</a>, <a href="https://docs.google.com/presentation/d/1w__6jKhNgzwT66oMcUyKgvGJEwgOVMlH-12WAuAHaKc/edit?usp=sharing">Slides</a></li><li><b><i>October 2022:</i></b> ECCV’22 Workshop on 3D Perception in Autonomous Driving <a href="https://innoviz.tech/eccv-speakers">Url</a> <a href="tbd">Slides</a></li><li><b><i>October 2022:</i></b> ECCV’22 Workshop on Cross-Modal Human-Robot Interaction <a href="https://cross-modal-human-robot-interaction.github.io/speakers.html">Url</a> <a href="tbd">Slides</a></li><li><b><i>June 2022:</i></b> Slovenian data-science meetup talk (in Slovenian): <a href="https://docs.google.com/presentation/d/1snIpyxQaFQYvSvx_V56I1VVqow-3UlSlDw9gIEH8IsI/edit?usp=sharing">Slides</a></li><li><b><i>April 2022:</i></b> UT Austin AI colloquium, <b>Unifying Segmentation, Tracking, and Forecasting</b>, <a href="https://docs.google.com/presentation/d/1us6m0LwJ3_Ai04yS13ztskCwQ4GCU-Vb5uLP41qs9Vc/edit?usp=sharin"g> Slides</a></li><li><b><i>September 2021:</i></b> ICCV’21 Workshop on 3D Object Detection from Images, <b>4D Panoptic LiDAR Segmentation</b>, <a href="https://docs.google.com/presentation/d/1NdMZ1ZHlGMkUBjvLEjcNyR6sWovXPO2OJEF8QrDNqmM/edit?usp=sharing"g> Slides</a></li><li><b><i>July 2021:</i></b> RSS 2021 on Workshop on Behavioral Inference of Remotely Sensed Multi-agent Systems invited talk, <b>Tracking Every Object and Pixel</b>, <a href="https://docs.google.com/presentation/d/1MvjuIpNDPFLveJDWb6IU2QjeKHHhkZv0jk2aFgOdAI0/edit?usp=sharing"> Slides</a></li><li><b><i>July 2021:</i></b> RSS 2021 Workshop on Perception and Control for Autonomous Navigation in Crowded, Dynamic Environments invited talk, <b>Tracking Every Object and Pixel</b>, <a href="https://docs.google.com/presentation/d/1osEVQOwyByzjnrFJ8GD13GY6A2BTufcdZAgX9RPs74k/edit?usp=sharing"> Slides</a>, <a href="https://youtu.be/Crur1kKLFmA"> Talk</a></li><li><b><i>June 2021:</i></b> <a href="https://jrdb.erc.monash.edu/workshops/cvpr2021">CVPR'21 JackRobbot dataset and benchmark (JRDB) workshop talk,</a> <b>Tracking Every Object and Pixel</b>, <a href="https://docs.google.com/presentation/d/1AdtfJFmpbB1Hi6EpcQMzB5ylfk03p2-_DI7ZEgxmz8o/edit?usp=sharing">Slides</a></li><li><b><i>April 2021:</i></b> Cornell Robotics Seminar, <a href="https://docs.google.com/presentation/d/1r19VsfVVRboE-kb4dVCUqz8xNeWKtmxqHDNj27kR4Us/edit?usp=sharing">Slides</a></li><li><b><i>September 2020:</i></b> University of Bonn - Research talk, <a href="https://docs.google.com/presentation/d/1GCDF-FBnXW9i0NrSTqB20BRgpN3uOMxWgGwp3XVzc5A/edit?usp=sharing">Slides</a></li><li><b><i>June 2019:</i></b> RWTH Aachen University - Thesis Defense, <a href="https://docs.google.com/presentation/d/e/2PACX-1vQ4POvCXfiL1fW5zoxpfEoOJDFNUxgbvoGKLWXuyj2rEz6I4xSeYCp9mhIFrxLfM9ckpg8zOlcDlBZ4/pub?start=false&loop=false&delayms=3000">Slides</a></li><li><b><i>June 2019:</i></b> Georgia Tech - Research Talk, <a href="https://docs.google.com/presentation/d/e/2PACX-1vSelDf9pZ8yAeMYGbndWO_OyqR2faPCum5G1vWKvGicB7s7E0LG3oAMD7iSTWiG_-RDT6TwAbg6fb5I/pub?start=false&loop=false&delayms=3000">Slides</a></li><li><b><i>March 2019:</i></b> Carnegie Mellon University VASC Seminar, <a href="https://docs.google.com/presentation/d/e/2PACX-1vTKleY9LI8z4Tc2FIXVd0woFKqMlkXjGwPpNOZTtw1VDUCucoxce4FFCe0Mi6g_wIrtCRv6kv3tw3Wk/pub?start=false&loop=false&delayms=3000">Slides</a></li></ul> <h2>Teaching</h2> <ul><li><b><i>Summer 2022:</i></b> Lecturer for <a href="https://dvl.in.tum.de/teaching/cv3dst-ss22/">IN2375: Computer Vision III: Detection, Segmentation and Tracking (CV3DST)</a> (TU Munich)</li><li><b><i>Summer 2022:</i></b> Lecturer for <a href="https://dvl.in.tum.de/teaching/i2dl-ss22/">IN2346: Introduction to Deep Learning (I2DL)</a> (TU Munich)</li><li><b><i>Winter 2020/21:</i></b> <a href="https://dvl.in.tum.de/teaching/fundalg-ws20/">IN2157: Fundamental Algorithms </a> (TU Munich, lecturer)</li><li><b><i>Summer 2019/20:</i></b> IN2375: Computer Vision 3: Detection, Segmentation and Tracking (TU Munich, guest lecturer) <a href="https://t.co/tTTtpKN2pi?amp=1">Lectures avalible on YouTube</a></li><li><b><i>Winter 2019/20:</i></b> IN2157: Fundamental Algorithms (TU Munich, lecturer) <a href="https://www.moodle.tum.de/course/view.php?id=52150">TUM Moodle</a>, IN2375: Computer Vision 3: Detection, Segmentation and Tracking (TU Munich, guest lecturer)</li><li><b><i>Summer 2016/17:</i></b> Machine Learning (RWTH Aachen, exercise class)</li><li><b><i>Winter 2014/15:</i></b> Computer Vision (RWTH Aachen, execrise class)</li><li><b><i>Winter 2013/14:</i></b> Machine Learning (RWTH Aachen, exercise class)</li></ul> <h2>Publications</h2> <div class="thebibliography"><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/anirudh_ijcv_2024.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Chakravarthy, M. Ganesina, P. Hu, L. Leal-Taixe, S. Kong, D. Ramanan, A. Osep: <b>Lidar Panoptic Segmentation in an Open World</b>, International Journal of Computer Vision (IJCV), 2024.  </br> <a href="https://github.com/g-meghana-reddy/open-world-panoptic-segmentation" target="_blank">code</a> <a href="https://arxiv.org/abs/2409.14273" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/osep_arxiv_2024.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Ošep, T. Meinhardt, F. Ferroni, N. Peri, D. Ramanan, L. Leal-Taixe: <b>Better Call SAL: Towards Learning to Segment Anything in Lidar</b>, European Conference on Computer Vision (ECCV), 2024.  </br> <a href="https://research.nvidia.com/labs/dvl/projects/sal" target="_blank">page</a> <a href="https://youtu.be/IwFl8l8_P5U?si=C7GEYncqfrvcy_DY" target="_blank">video</a> <a href="https://arxiv.org/abs/2403.13129" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/seidenschwarz_cvpr_2024.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> J. Seidenschwarz, A. Ošep, F. Ferroni, S. Lucey, L. Leal-Taixe: <b>What Moves Together Belongs Together</b>, Conference on Computer Vision and Pattern Recognition (CVPR), 2024.  </br> <a href="https://github.com/dvl-tum/SeMoLi" target="_blank">code</a> <a href="https://research.nvidia.com/labs/dvl/projects/semoli" target="_blank">page</a> <a href="https://arxiv.org/abs/2402.19463" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/saltori_iccv_2023.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> C. Saltori, A. Ošep, E. Ricci, L. Leal-Taixé: <b>Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation</b>, International Conference on Computer Vision (ICCV), 2023.  </br> <a href="https://www.youtube.com/watch?v=IKOppoT8cIE&ab_channel=CristianoSaltori" target="_blank">video</a> <a href="https://github.com/saltoricristiano/LiDOG" target="_blank">code</a> <a href="https://arxiv.org/abs/2304.11705" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/agarwalla_iros_2023.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Agarwalla, X. Huang, J. Ziglar, F. Ferroni, L. Leal-Taixé, J. Hays, A. Ošep, D. Ramanan: <b>Lidar Panoptic Segmentation and Tracking without Bells and Whistles</b>, International Conference on Intelligent Robots and Systems (IROS), 2023.  </br> <a href="https://mostlps.github.io/" target="_blank">page</a> <a href="https://mostlps.github.io/assets/paper.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/wu_arxiv_2023.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> X. Wu, K. Lau, F. Ferroni, A. Ošep, D. Ramanan: <b>Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images</b>, Conference on Computer Vision and Pattern Recognition (CVPR), 2023.  </br> <a href="https://drive.google.com/file/d/1o0rrnxCcVRtqJNzlM7v9GW-XBPiGqet4/view" target="_blank">poster</a> <a href="https://youtu.be/18VtggvpynY" target="_blank">video</a> <a href="https://pix2map.github.io" target="_blank">page</a> <a href="https://arxiv.org/abs/2301.04224" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/fomenko_neurips_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> V. Fomenko, I. Elezi, D. Ramanan, L. Leal-Taixé, A. Ošep: <b>Learning to Discover and Detect Objects</b>, Neural Information Processing Systems (NeurIPS), 2022.  </br> <a href="https://vlfom.github.io/RNCDL/" target="_blank">page</a> <a href="https://drive.google.com/file/d/1gDwgv-nKM3btpCqPU98RdC7ztuHu2cYF/view" target="_blank">poster</a> <a href="https://www.youtube.com/watch?v=zWpUnXNplfQ" target="_blank">video</a> <a href="https://github.com/vlfom/RNCDL" target="_blank">code</a> <a href="https://arxiv.org/abs/2210.10774" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/dendorfer_neurips_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> P. Dendorfer, V. Yugay, A. Ošep, L. Leal-Taixé: <b>Quo Vadis: Is Trajectory Forecasting the Key Towards
Long-Term Multi-Object Tracking?</b>, Neural Information Processing Systems (NeurIPS), 2022.  </br> <a href="https://www.youtube.com/watch?v=LcjNoLtieBw" target="_blank">video</a> <a href="https://github.com/dendorferpatrick/QuoVadis" target="_blank">code</a> <a href="https://arxiv.org/abs/2210.07681" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/kim_eccv_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Kim, G. Brasó, A. Ošep, L. Leal-Taixé: <b>PolarMOT: How far can geometric relations take us in 3D multi-object tracking?</b>, European Conference on Computer Vision (ECCV), 2022.  </br> <a href="https://youtu.be/kYOInmOeMbM" target="_blank">video</a> <a href="https://polarmot.github.io/figures/poster.jpg" target="_blank">poster</a> <a href="https://polarmot.github.io/" target="_blank">code</a> <a href="https://arxiv.org/abs/2208.01957" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/zhou_eccv_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> Q. Zhou, S. Agostinho, A. Ošep, L. Leal-Taixé: <b>Is Geometry Enough for Matching in Visual Localization?</b>, European Conference on Computer Vision (ECCV), 2022.  </br> <a href="https://arxiv.org/pdf/2203.12979.pdf" target="_blank">paper</a> <a href="https://github.com/dvl-tum/gomatch" target="_blank">code</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/nunes_ral_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> L. Nunes, X. Chen, R. Marcuzzi, A. Ošep, L. Leal-Taixé, C. Stachniss, J. Behley: <b>Unsupervised Class-Agnostic Instance Segmentation of 3D LiDAR Data for Autonomous Vehicles</b>, IEEE Robotics and Automation Letters (RA-L), 2022.  </br> <a href="https://github.com/PRBonn/3DUIS" target="_blank">code</a> <a href="https://www.ipb.uni-bonn.de/pdfs/nunes2022ral-iros.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/gladkova_iros_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> M. Gladkova, N. Korobov, N. Demmel, A. Ošep, L. Leal-Taixé, D. Cremers: <b>DirectTracker: 3D Multi-Object Tracking Using Direct Image Alignment and Photometric Bundle Adjustment</b>, International Conference on Intelligent Robots and Systems (IROS), 2022.  </br> <a href="https://youtu.be/hpm7bkGLdjs" target="_blank">video</a> <a href="https://arxiv.org/abs/2209.14965" target="_blank">paper</a> <a href="https://vision.in.tum.de/research/vslam/directtracker" target="_blank">page</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/peri_cvpr_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> N. Peri, J. Luiten, M. Li, A. Ošep, L. Leal-Taixé, D. Ramanan: <b>Forecasting from LiDAR via Future Object Detection</b>, Conference on Computer Vision and Pattern Recognition (CVPR), 2022.  </br> <a href="https://github.com/neeharperi/FutureDet" target="_blank">code</a> <a href="https://arxiv.org/abs/2203.16297" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/kolmet_cvpr_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> M. Kolmet, Q. Zhou, A. Ošep, L. Leal-Taixé: <b>Text2Pos: Text-to-point-cloud cross-modal localization</b>, Conference on Computer Vision and Pattern Recognition (CVPR), 2022.  </br> <a href="https://text2pos.github.io/" target="_blank">code</a> <a href="https://arxiv.org/abs/2203.15125" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/liu_cvpr_2022.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> Y. Liu, I. Zulfikar, J. Luiten, A. Dave, D. Ramanan, B. Leibe, A. Ošep, L. Leal-Taixé: <b>Opening up Open-World Tracking</b>, Conference on Computer Vision and Pattern Recognition (CVPR), oral, 2022.  </br> <a href="https://openworldtracking.github.io/" target="_blank">code</a> <a href="https://arxiv.org/abs/2104.11221" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/agostinho_iccv_2021.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> S. Agostinho, A. Ošep, A. Del Bue, L. Leal-Taixé: <b>(Just) A Spoonful of Refinements Helps the Registration Error Go Down</b>, International Conference on Computer Vision (ICCV) (oral), 2021.  </br> <a href="https://github.com/SergioRAgostinho/just-a-spoonful" target="_blank">code</a> <a href="https://arxiv.org/abs/2108.03257" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/fabri_iccv_2021.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> M. Fabbri, G. Brasó, G. Maugeri, A. Ošep, R. Gasparini, O. Cetintas, S. Calderara, L. Leal-Taixé, R. Cucchiara: <b>MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?</b>, International Conference on Computer Vision (ICCV), 2021.  </br> <a href="https://youtu.be/0k8M1lT6KcE" target="_blank">video</a> <a href="https://arxiv.org/abs/2108.09518" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/ayguen_cvpr_2021.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> M. Aygün, A. Ošep, M. Weber, M. Maximov, C. Stachniss, J. Behley, L. Leal-Taixé: <b>4D Panoptic LiDAR Segmentation</b>, Conference on Computer Vision and Pattern Recognition (CVPR), 2021.  </br> <a href="https://github.com/mehmetaygun/4d-pls" target="_blank">code</a> <a href="https://drive.google.com/file/d/1X72Cv-YNEKK0KbtSoYjsZ9En2JN7tEj2/view?usp=sharing" target="_blank">poster</a> <a href="https://www.youtube.com/watch?v=29ft_i78mDE&ab_channel=DynamicVisionandLearningGroup" target="_blank">video</a> <a href="https://arxiv.org/abs/2102.12472" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/kim_icra_2021.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Kim, A. Ošep, L. Leal-Taixé: <b>EagerMOT: 3D Multi-Object Tracking via Sensor Fusion</b>, IEEE International Conference on Robotics and
Automation (ICRA), 2021.  </br> <a href="https://github.com/aleksandrkim61/EagerMOT" target="_blank">code</a> <a href="https://youtu.be/k8pKpvbenoM" target="_blank">video</a> <a href="https://arxiv.org/abs/2104.14682" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/weber_neurips_2021.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> M. Weber, J. Xie, M. Collins, Y. Zhu, P. Voigtlaender, H. Adam, B. Green, A. Geiger, B. Leibe, D. Cremers, A. Os̆ep, L. Leal-Taixé, L. Chen: <b>STEP: Segmenting and Tracking Every Pixel</b>, NeurIPS Benchmarks and Datasets, 2022.  </br> <a href="https://github.com/google-research/deeplab2" target="_blank">code</a> <a href="https://arxiv.org/abs/2102.11859" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/dendorfer_accv_2020.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> P. Dendorfer, A. Ošep, L. Leal-Taixé: <b>Goal-GAN: Multimodal Trajectory Prediction Based on Goal Position Estimation</b>, Asian Conference on Computer Vision (ACCV), 2020.  </br> <a href="https://dendorferpatrick.github.io/GoalGAN/" target="_blank">page</a> <a href="https://github.com/dendorferpatrick/GoalGAN" target="_blank">code</a> <a href="https://youtu.be/SoMbBNpAQOw" target="_blank">video</a> <a href="https://arxiv.org/pdf/2010.01114" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/dendorfer_ijcv_2020.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> P. Dendorfer, A. Ošep, A. Milan, K. Schindler, D. Cremers, I. Reid, S. Leal-Taixé: <b>MOTChallenge: A Benchmark for Single-camera Multiple Target Tracking</b>, International Journal of Computer Vision (IJCV), 2020.  </br> <a href="https://arxiv.org/abs/2010.07548" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/luiten_ijcv_2020.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> J. Luiten, A. Ošep, P. Dendorfer, P. Torr, A. Geiger, L. Leal-Taixé, B. Leibe: <b>HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking</b>, International Journal of Computer Vision (IJCV), 2020.  </br> <a href="https://github.com/JonathonLuiten/TrackEval" target="_blank">code</a> <a href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1" target="_blank">blog</a> <a href="https://arxiv.org/pdf/2009.07736.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/mahadevan_bmvc_2020.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> S. Mahadevan*, A. Athar*, A. Ošep, S. Hennen, L. Leal-Taixé, B. Leibe: <b>Making a Case for 3D Convolutions for Object Segmentation in Videos</b>, British Machine Vision Conference (BMVC), 2020.  </br> <a href="https://github.com/sabarim/3DC-Seg" target="_blank">code</a> <a href="https://www.youtube.com/watch?v=vU3g2mpL1XA&ab_channel=RWTHVision" target="_blank">video</a> <a href="https://arxiv.org/pdf/2008.11516" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/athar_eccv_2020.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Athar*, S. Mahadevan*, A. Ošep, L. Leal-Taixé, B. Leibe: <b>STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos</b>, European Conference on Computer Vision (ECCV), 2020.  </br> <a href="https://github.com/sabarim/STEm-Seg" target="_blank">code</a> <a href="https://www.youtube.com/watch?v=E2Z-1HNO934&ab_channel=RWTHVision" target="_blank">video</a> <a href="https://arxiv.org/pdf/2003.08429.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/xu_cvpr_2020.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> Y. Xu, A. Ošep, Y. Ban, R. Horaud, L. Leal-Taixé, X. Alameda-Pineda: <b>How To Train Your Deep Multi-Object Tracker</b>, Conference on Computer Vision and Pattern Recognition (CVPR), 2020.  </br> <a href="https://github.com/yihongXU/deepMOT" target="_blank">code</a> <a href="https://www.youtube.com/watch?v=jj17HVNl700&t=881s&ab_channel=DynamicVisionandLearningGroup" target="_blank">video</a> <a href="https://arxiv.org/abs/1906.06618" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/gross_3dv_2019.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> J. Gross, A. Ošep, B. Leibe: <b>AlignNet-3D for Fast Point Cloud Registration of Partially Observed Objects</b>, International Conference on 3D Vision (3DV), 2019.  </br> <a href="https://www.vision.rwth-aachen.de/page/alignnet" target="_blank">code</a> <a href="https://drive.google.com/open?id=1HiXC-p5v_gpr2CYz-WCA3RDCuvPEeAdf" target="_blank">video</a> <a href="https://vision.rwth-aachen.de/media/papers/189/poster-alignnet.pdf" target="_blank">poster</a> <a href="https://arxiv.org/abs/1910.04668" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/voigtlaender_cvpr_2019.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> P. Voigtlaender, M. Krause, A. Ošep, J. Luiten, B. Sekar, A. Geiger, B. Leibe: <b>{MOTS}: Multi-Object Tracking and Segmentation</b>, Conference on Computer Vision and Pattern Recognition (CVPR), 2019.  </br> <a href="https://www.vision.rwth-aachen.de/page/mots" target="_blank">code</a> <a href="https://vision.rwth-aachen.de/media/papers/178/MOTS_video.mp4" target="_blank">video</a> <a href="https://arxiv.org/abs/1902.03604" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/osep_icra_2020.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Ošep, P. Voigtlaender, M. Weber, J. Luiten, B. Leibe: <b>4D Generic Video Object Proposals</b>, International Conference on Robotics and Automation (ICRA), 2020.  </br> <a href="https://github.com/aljosaosep/4DGVT" target="_blank">code</a> <a href="https://youtu.be/C5phv--Fhes" target="_blank">teaser</a> <a href="https://youtu.be/fu2xP7YpZ54" target="_blank">video</a> <a href="https://arxiv.org/pdf/1901.09260" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/osep_icra_2019.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Ošep, P. Voigtlaender, J. Luiten, S. Breuers, B. Leibe: <b>Large-Scale Object Mining for Object Discovery from Unlabeled Video</b>, International Conference on Robotics and Automation (ICRA), 2019.  </br> <a href="https://youtu.be/r3o0FuNzfb0" target="_blank">video</a> <a href="https://arxiv.org/pdf/1903.00362" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/osep_icra_2018.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Ošep, W. Mehner, P. Voigtlaender, B. Leibe: <b>Track, then Decide: Category-Agnostic Vision-based Multi-Object Tracking</b>, International Conference on Robotics and Automation (ICRA), 2018.  </br> <a href="https://github.com/aljosaosep/camot" target="_blank">code</a> <a href="https://www.youtube.com/watch?v=HYXzHuD4AKI&ab_channel=RWTHVision" target="_blank">video</a> <a href="https://arxiv.org/abs/1712.07920" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/osep_icra_2019.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Ošep, P. Voigtlaender, J. Luiten, S. Breuers, B. Leibe: <b>Towards Large-Scale Video Object Mining</b>, ECCV 2018 Workshop on Interactive and Adaptive Learning in an Open World, 2018.  </br> <a href="https://arxiv.org/pdf/1809.07316" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/osep_icra_2017.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Ošep, W. Mehner, M. Mathias, B. Leibe: <b>Combined Image- and World-Space Tracking in Traffic Scenes</b>, International Conference on Robotics and Automation (ICRA), 2017.  </br> <a href="https://youtu.be/9_U5shk9fDk" target="_blank">teaser</a> <a href="https://github.com/aljosaosep/ciwt" target="_blank">code</a> <a href="https://youtu.be/TCdgUI5Xmus" target="_blank">video</a> <a href="https://arxiv.org/abs/1809.07357" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/klostermann_bmvc_2016.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> D. Klostermann, A. Ošep, J. Stueckler, B. Leibe: <b>Unsupervised Learning of Shape-Motion Patterns for Objects in Urban Street Scenes</b>, British Machine Vision Conference (BMVC), 2016.  </br> <a href="https://vision.rwth-aachen.de/media/papers/bmvc16_klostermann_supplementary.mp4" target="_blank">video</a> <a href="https://www.vision.rwth-aachen.de/media/papers/bmvc16_klostermann_final.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/kochanov_iros_2016.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> D. Kochanov, A. Ošep, J. Stueckler, B. Leibe: <b>Scene Flow Propagation for Semantic Mapping and Object Discovery in Dynamic Street Scenes</b>, International Conference on Intelligent Robots and Systems (IROS), 2016.  </br> <a href="https://vision.rwth-aachen.de/media/papers/supplemental_video.mp4" target="_blank">video</a> <a href="https://www.vision.rwth-aachen.de/media/papers/paper_compressed.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/osep_icra_2016.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> A. Ošep, A. Hermans, F. Engelmann, D. Klostermann, M. Mathias, B. Leibe: <b>Multi-Scale Object Candidates for Generic Object Tracking in Street Scenes</b>, International Conference on Robotics and Automation (ICRA), 2016.  </br> <a href="https://www.vision.rwth-aachen.de/media/papers/osep_ICRA16_paper.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/mitzel_icra_2015.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> D. Mitzel, J. Diesel, A. Ošep, U. Rafi, B. Leibe: <b>A Fixed-Dimensional 3D Shape Representation for Matching Partially Observed Objects in Street Scenes</b>, International Conference on Robotics and Automation (ICRA), 2015.  </br> <a href="https://www.vision.rwth-aachen.de/media/papers/mitzel15icra_3d_shape_representation.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/weinmann_iccv_2013.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> M. Weinmann, A. Ošep, R. Ruiters, R. Klein: <b>Multi-View Normal Field Integration for 3D Reconstruction of Mirroring Objects</b>, International Conference on Computer Vision (ICCV), 2013.  </br> <a href="https://www.vision.rwth-aachen.de/media/papers/weinmann_reconstruction_of_mirroring_objects_iccv2013.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /><div><div style="float: left; margin: 5px 20px 10px 0px;"><img src="img/thumb/weinmann_bmvc_2012.jpg" width="200" height="200" style="border-radius: 8px;"/></div><div><p class="bibitem" ><span class="biblabel"></span> M. Weinmann, R. Ruiters, A. Ošep, C. Schwartz, R. Klein: <b>Fusing Structured Light Consistency and Helmholtz Normals for 3D Reconstruction</b>, British Machine Vision Conference (BMVC), 2012.  </br> <a href="https://www.vision.rwth-aachen.de/media/papers/weinmann-2012-3DReconstruction.pdf" target="_blank">paper</a>  </p></div></div><br clear="all" /></div> </body></html>
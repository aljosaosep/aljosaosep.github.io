<!DOCTYPE html><html lang="en"><head><title>Aljosa's Web Corner</title><link rel="stylesheet" href=https://latex.now.sh/style.min.css /></head><body><h2>Publications</h2> <div class="thebibliography"><p class="bibitem" ><span class="biblabel"></span> L. Nunes, X. Chen, R. Marcuzzi, A. Ošep, L. Leal-Taixé, C. Stachniss, J. Behley. Unsupervised Class-Agnostic Instance Segmentation of 3D LiDAR Data for Autonomous Vehicles. In IEEE Robotics and Automation Letters (RA-L), 2022. </br> <a href="https://www.ipb.uni-bonn.de/pdfs/nunes2022ral-iros.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> M. Gladkova, N. Korobov, N. Demmel, A. Ošep, L. Leal-Taixé, D. Cremers. DirectTracker: 3D Multi-Object Tracking Using Direct Image Alignment and Photometric Bundle Adjustment. In International Conference on Intelligent Robots and Systems (IROS), 2022. </br>  </p><p class="bibitem" ><span class="biblabel"></span> A. Kim, G. Brasó, A. Ošep, L. Leal-Taixé. How far can geometric relations take us in 3D multi-object tracking?. In European Conference on Computer Vision (ECCV), 2022. </br> <a href="https://polarmot.github.io/" target="_blank">code</a> <a href="https://arxiv.org/abs/2208.01957" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> Q. Zhou, S. Agostinho, A. Ošep, L. Leal-Taixé. Is Geometry Enough for Matching in Visual Localization?. In European Conference on Computer Vision (ECCV), 2022. </br> <a href="https://arxiv.org/pdf/2203.12979.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> N. Peri, J. Luiten, M. Li, A. Ošep, L. Leal-Taixé, D. Ramanan. Forecasting from LiDAR via Future Object Detection. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. </br> <a href="https://github.com/neeharperi/FutureDet" target="_blank">code</a> <a href="https://arxiv.org/abs/2203.16297" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> M. Kolmet, Q. Zhou, A. Ošep, L. Leal-Taixé. Towards cross-modal pose localization from text-based position descriptions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. </br> <a href="https://text2pos.github.io/" target="_blank">code</a> <a href="https://arxiv.org/abs/2203.15125" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> Y. Liu, I. Zulfikar, J. Luiten, A. Dave, D. Ramanan, B. Leibe, A. Ošep, L. Leal-Taixé. Opening up Open-World Tracking. In Conference on Computer Vision and Pattern Recognition (CVPR), oral, 2022. </br> <a href="https://openworldtracking.github.io/" target="_blank">code</a> <a href="https://arxiv.org/abs/2104.11221" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> S. Agostinho, A. Ošep, A. Del Bue, L. Leal-Taixé. (Just) A Spoonful of Refinements Helps the Registration Error Go Down. In International Conference on Computer Vision (ICCV) (oral), 2021. </br> <a href="https://github.com/SergioRAgostinho/just-a-spoonful" target="_blank">code</a> <a href="https://arxiv.org/abs/2108.03257" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> M. Fabbri, G. Brasó, G. Maugeri, A. Ošep, R. Gasparini, O. Cetintas, S. Calderara, L. Leal-Taixé, R. Cucchiara. MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?. In International Conference on Computer Vision (ICCV), 2021. </br> <a href="https://youtu.be/0k8M1lT6KcE" target="_blank">video</a> <a href="https://arxiv.org/abs/2108.09518" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> M. Aygün, A. Ošep, M. Weber, M. Maximov, C. Stachniss, J. Behley, L. Leal-Taixé. 4D Panoptic LiDAR Segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. </br> <a href="https://github.com/mehmetaygun/4d-pls" target="_blank">code</a> <a href="https://drive.google.com/file/d/1X72Cv-YNEKK0KbtSoYjsZ9En2JN7tEj2/view?usp=sharing" target="_blank">poster</a> <a href="https://www.youtube.com/watch?v=29ft_i78mDE&ab_channel=DynamicVisionandLearningGroup" target="_blank">video</a> <a href="https://arxiv.org/abs/2102.12472" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> A. Kim, A. Ošep, L. Leal-Taixé. EagerMOT: 3D Multi-Object Tracking via Sensor Fusion. In IEEE International Conference on Robotics and
Automation (ICRA), 2021. </br> <a href="https://github.com/aleksandrkim61/EagerMOT" target="_blank">code</a> <a href="https://youtu.be/k8pKpvbenoM" target="_blank">video</a> <a href="https://arxiv.org/abs/2104.14682" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> M. Weber, J. Xie, M. Collins, Y. Zhu, P. Voigtlaender, H. Adam, B. Green, A. Geiger, B. Leibe, D. Cremers, A. Os̆ep, L. Leal-Taixé, L. Chen. STEP: Segmenting and Tracking Every Pixel. In NeurIPS Benchmarks and Datasets, 2022. </br> <a href="https://github.com/google-research/deeplab2" target="_blank">code</a> <a href="https://arxiv.org/abs/2102.11859" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> P. Dendorfer, A. Ošep, L. Leal-Taixé. Goal-GAN: Multimodal Trajectory Prediction Based on Goal Position Estimation. In Asian Conference on Computer Vision (ACCV), 2020. </br> <a href="https://dendorferpatrick.github.io/GoalGAN/" target="_blank">page</a> <a href="https://github.com/dendorferpatrick/GoalGAN" target="_blank">code</a> <a href="https://youtu.be/SoMbBNpAQOw" target="_blank">video</a> <a href="https://arxiv.org/pdf/2010.01114" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> P. Dendorfer, A. Ošep, A. Milan, K. Schindler, D. Cremers, I. Reid, S. Leal-Taixé. MOTChallenge: A Benchmark for Single-camera Multiple Target Tracking. In International Journal of Computer Vision (IJCV), 2020. </br> <a href="https://arxiv.org/abs/2010.07548" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> J. Luiten, A. Ošep, P. Dendorfer, P. Torr, A. Geiger, L. Leal-Taixé, B. Leibe. HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking. In International Journal of Computer Vision (IJCV), 2020. </br> <a href="https://github.com/JonathonLuiten/TrackEval" target="_blank">code</a> <a href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1" target="_blank">blog</a> <a href="https://arxiv.org/pdf/2009.07736.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> S. Mahadevan*, A. Athar*, A. Ošep, S. Hennen, L. Leal-Taixé, B. Leibe. Making a Case for 3D Convolutions for Object Segmentation in Videos. In British Machine Vision Conference (BMVC), 2020. </br> <a href="https://github.com/sabarim/3DC-Seg" target="_blank">code</a> <a href="https://www.youtube.com/watch?v=vU3g2mpL1XA&ab_channel=RWTHVision" target="_blank">video</a> <a href="https://arxiv.org/pdf/2008.11516" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> A. Athar*, S. Mahadevan*, A. Ošep, L. Leal-Taixé, B. Leibe. STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos. In European Conference on Computer Vision (ECCV), 2020. </br> <a href="https://github.com/sabarim/STEm-Seg" target="_blank">code</a> <a href="https://www.youtube.com/watch?v=E2Z-1HNO934&ab_channel=RWTHVision" target="_blank">video</a> <a href="https://arxiv.org/pdf/2003.08429.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> Y. Xu, A. Ošep, Y. Ban, R. Horaud, L. Leal-Taixé, X. Alameda-Pineda. How To Train Your Deep Multi-Object Tracker. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. </br> <a href="https://github.com/yihongXU/deepMOT" target="_blank">code</a> <a href="https://www.youtube.com/watch?v=jj17HVNl700&t=881s&ab_channel=DynamicVisionandLearningGroup" target="_blank">video</a> <a href="https://arxiv.org/abs/1906.06618" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> J. Gross, A. Ošep, B. Leibe. AlignNet-3D for Fast Point Cloud Registration of Partially Observed Objects. In International Conference on 3D Vision (3DV), 2019. </br> <a href="https://www.vision.rwth-aachen.de/page/alignnet" target="_blank">code</a> <a href="https://drive.google.com/open?id=1HiXC-p5v_gpr2CYz-WCA3RDCuvPEeAdf" target="_blank">video</a> <a href="https://vision.rwth-aachen.de/media/papers/189/poster-alignnet.pdf" target="_blank">poster</a> <a href="https://arxiv.org/abs/1910.04668" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> P. Voigtlaender, M. Krause, A. Ošep, J. Luiten, B. Sekar, A. Geiger, B. Leibe. {MOTS}: Multi-Object Tracking and Segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. </br> <a href="https://www.vision.rwth-aachen.de/page/mots" target="_blank">code</a> <a href="https://vision.rwth-aachen.de/media/papers/178/MOTS_video.mp4" target="_blank">video</a> <a href="https://arxiv.org/abs/1902.03604" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> A. Ošep, P. Voigtlaender, M. Weber, J. Luiten, B. Leibe. 4D Generic Video Object Proposals. In International Conference on Robotics and Automation (ICRA), 2020. </br> <a href="https://github.com/aljosaosep/4DGVT" target="_blank">code</a> <a href="https://youtu.be/C5phv--Fhes" target="_blank">teaser</a> <a href="https://youtu.be/fu2xP7YpZ54" target="_blank">video</a> <a href="https://arxiv.org/pdf/1901.09260" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> A. Ošep, P. Voigtlaender, J. Luiten, S. Breuers, B. Leibe. Large-Scale Object Mining for Object Discovery from Unlabeled Video. In International Conference on Robotics and Automation (ICRA), 2019. </br> <a href="https://youtu.be/r3o0FuNzfb0" target="_blank">video</a> <a href="https://arxiv.org/pdf/1903.00362" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> A. Ošep, W. Mehner, P. Voigtlaender, B. Leibe. Track, then Decide: Category-Agnostic Vision-based Multi-Object Tracking. In International Conference on Robotics and Automation (ICRA), 2018. </br> <a href="https://github.com/aljosaosep/camot" target="_blank">code</a> <a href="https://www.youtube.com/watch?v=HYXzHuD4AKI&ab_channel=RWTHVision" target="_blank">video</a> <a href="https://arxiv.org/abs/1712.07920" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> A. Ošep, P. Voigtlaender, J. Luiten, S. Breuers, B. Leibe. Towards Large-Scale Video Object Mining. In ECCV 2018 Workshop on Interactive and Adaptive Learning in an Open World, 2018. </br> <a href="https://arxiv.org/pdf/1809.07316" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> A. Ošep, W. Mehner, M. Mathias, B. Leibe. Combined Image- and World-Space Tracking in Traffic Scenes. In International Conference on Robotics and Automation (ICRA), 2017. </br> <a href="https://youtu.be/9_U5shk9fDk" target="_blank">teaser</a> <a href="https://github.com/aljosaosep/ciwt" target="_blank">code</a> <a href="https://youtu.be/TCdgUI5Xmus" target="_blank">video</a> <a href="https://arxiv.org/abs/1809.07357" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> D. Klostermann, A. Ošep, J. Stueckler, B. Leibe. Unsupervised Learning of Shape-Motion Patterns for Objects in Urban Street Scenes. In British Machine Vision Conference (BMVC), 2016. </br> <a href="https://vision.rwth-aachen.de/media/papers/bmvc16_klostermann_supplementary.mp4" target="_blank">video</a> <a href="https://www.vision.rwth-aachen.de/media/papers/bmvc16_klostermann_final.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> D. Kochanov, A. Ošep, J. Stueckler, B. Leibe. Scene Flow Propagation for Semantic Mapping and Object Discovery in Dynamic Street Scenes. In International Conference on Intelligent Robots and Systems (IROS), 2016. </br> <a href="https://vision.rwth-aachen.de/media/papers/supplemental_video.mp4" target="_blank">video</a> <a href="https://www.vision.rwth-aachen.de/media/papers/paper_compressed.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> A. Ošep, A. Hermans, F. Engelmann, D. Klostermann, M. Mathias, B. Leibe. Multi-Scale Object Candidates for Generic Object Tracking in Street Scenes. In International Conference on Robotics and Automation (ICRA), 2016. </br> <a href="https://www.vision.rwth-aachen.de/media/papers/osep_ICRA16_paper.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> D. Mitzel, J. Diesel, A. Ošep, U. Rafi, B. Leibe. A Fixed-Dimensional 3D Shape Representation for Matching Partially Observed Objects in Street Scenes. In International Conference on Robotics and Automation (ICRA), 2015. </br> <a href="https://www.vision.rwth-aachen.de/media/papers/mitzel15icra_3d_shape_representation.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> M. Weinmann, A. Ošep, R. Ruiters, R. Klein. Multi-View Normal Field Integration for 3D Reconstruction of Mirroring Objects. In International Conference on Computer Vision (ICCV), 2013. </br> <a href="https://www.vision.rwth-aachen.de/media/papers/weinmann_reconstruction_of_mirroring_objects_iccv2013.pdf" target="_blank">paper</a>  </p><p class="bibitem" ><span class="biblabel"></span> M. Weinmann, R. Ruiters, A. Ošep, C. Schwartz, R. Klein. Fusing Structured Light Consistency and Helmholtz Normals for 3D Reconstruction. In British Machine Vision Conference (BMVC), 2012. </br> <a href="https://www.vision.rwth-aachen.de/media/papers/weinmann-2012-3DReconstruction.pdf" target="_blank">paper</a>  </p></div> </body></html>